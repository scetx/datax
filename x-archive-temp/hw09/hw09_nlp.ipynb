{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 09\n",
    "Sentiment Analysis on IMDB movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to Gradescope HW 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As you go through the notebook, you will encounter these main steps in the code: \n",
    "1. Reading of file labeledTrainData.tsv from data folder in a dataframe `train`.\n",
    "2. A function review_cleaner(train['review'],lemmatize,stem) which cleans the reviews in the input file.\n",
    "3. A function train_predict_sentiment(cleaned_reviews, y=train[\"sentiment\"],ngram=1,max_features=1000\n",
    "4. You will see a model has been trained on unigrams of the reviews without lemmatizing and stemming.\n",
    "5. Your task is in 5.TODO section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make compatible with Python 2 and Python 3\n",
    "from __future__ import print_function, division, absolute_import \n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset\n",
    "\n",
    "The labeled training data set consists of 25,000 IMDB movie reviews. There is also an unlabeled test set with 25,000 IMDB movie reviews. The sentiment of the reviews are binary, meaning an IMDB rating < 5 results in a sentiment score of 0, and a rating >=7 have a sentiment score of 1 (no reviews with score 5 or 6 are included in the analysis). No individual movie has more than 30 reviews.\n",
    "\n",
    "## File description\n",
    "\n",
    "* **labeledTrainData** - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review. \n",
    "\n",
    "* **testData** - The unlabeled test set. 25,000 rows containing an id, and text for each review. \n",
    "\n",
    "## Data columns\n",
    "* **id** - Unique ID of each review\n",
    "* **sentiment** - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "* **review** - Text of the review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data set statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         25000 non-null  object\n",
      " 1   sentiment  25000 non-null  int64 \n",
      " 2   review     25000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/lizhi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "\n",
    "import bs4 as bs\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.Preparing the dataset for classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a function called `review_cleaner` that reads in a review and:\n",
    "\n",
    "- Removes HTML tags (using beautifulsoup)\n",
    "- **Extract emoticons (emotion symbols, aka smileys :D )**\n",
    "- Removes non-letters (using regular expression)\n",
    "- Converts all words to lowercase letters and tokenizes them (using .split() method on the review strings, so that every word in the review is an element in a list)\n",
    "- Removes all the English stopwords from the list of movie review words\n",
    "- Join the words back into one string seperated by space, append the emoticons to the end\n",
    "\n",
    "**NOTE: Transform the list of stopwords to a set before removing the stopwords. I.e. assign `eng_stopwords = set(stopwords.words(\"english\"))`. Use the set to look up stopwords. This will speed up the computations A LOT (Python is much quicker when searching a set than a list).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_cleaner(reviews, lemmatize=True, stem=False):\n",
    "    '''\n",
    "    Clean and preprocess a review.\n",
    "\n",
    "    1. Remove HTML tags\n",
    "    2. Use regex to remove all special characters (only keep letters)\n",
    "    3. Make strings to lower case and tokenize / word split reviews\n",
    "    4. Remove English stopwords\n",
    "    5. Rejoin to one string\n",
    "    '''\n",
    "    ps = PorterStemmer()\n",
    "    wnl = WordNetLemmatizer()\n",
    "    cleaned_reviews=[]\n",
    "    \n",
    "    for i, review in enumerate(train['review']):\n",
    "        # print progress\n",
    "        if((i+1)%500 == 0):\n",
    "            print(\"Done with %d reviews\" %(i+1))\n",
    "            \n",
    "        #1. Remove HTML tags\n",
    "        review = bs.BeautifulSoup(review).text\n",
    "\n",
    "        #2. Use regex to find emoticons\n",
    "        emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', review)\n",
    "\n",
    "        #3. Remove punctuation\n",
    "        review = re.sub(\"[^a-zA-Z]\", \" \",review)\n",
    "\n",
    "        #4. Tokenize into words (all lower case)\n",
    "        review = review.lower().split()\n",
    "\n",
    "        #5. Remove stopwords\n",
    "        eng_stopwords = set(stopwords.words(\"english\"))\n",
    "            \n",
    "        clean_review=[]\n",
    "        for word in review:\n",
    "            if word not in eng_stopwords:\n",
    "                if lemmatize is True:\n",
    "                    word=wnl.lemmatize(word)\n",
    "                elif stem is True:\n",
    "                    if word == 'oed':\n",
    "                        continue\n",
    "                    word=ps.stem(word)\n",
    "                clean_review.append(word)\n",
    "\n",
    "        #6. Join the review to one sentence\n",
    "        \n",
    "        review_processed = ' '.join(clean_review+emoticons)\n",
    "        cleaned_reviews.append(review_processed)\n",
    "    \n",
    "\n",
    "    return(cleaned_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Function to train and validate a sentiment analysis model using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# CountVectorizer can actucally handle a lot of the preprocessing for us\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def train_predict_sentiment(cleaned_reviews, y=train[\"sentiment\"], ngram=1, max_features=1000):\n",
    "    '''This function will:\n",
    "    1. split data into train and test set.\n",
    "    2. get n-gram counts from cleaned reviews \n",
    "    3. train a random forest model using train n-gram counts and y (labels)\n",
    "    4. test the model on your test split\n",
    "    5. print accuracy of sentiment prediction on test and training data\n",
    "    6. print confusion matrix on test data results\n",
    "\n",
    "    To change n-gram type, set value of ngram argument\n",
    "    To change the number of features you want the countvectorizer to generate, set the value of max_features argument'''\n",
    "\n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer is scikit-learn's bag of words tool, here we show more keywords \n",
    "    vectorizer = CountVectorizer(ngram_range=(1, ngram), analyzer = \"word\",\n",
    "                                 tokenizer = None,\n",
    "                                 preprocessor = None,\n",
    "                                 stop_words = None,\n",
    "                                 max_features = max_features) \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(cleaned_reviews, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarraty() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train).toarray()\n",
    "    test_bag = vectorizer.transform(X_test).toarray()\n",
    "    # print('TOP 20 FEATURES ARE: ',(vectorizer.get_feature_names()[:20]))\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 75 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    print(\" The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    print()\n",
    "    print('CONFUSION MATRIX:')\n",
    "    print('         Predicted')\n",
    "    print('          neg pos')\n",
    "    print(' Actual')\n",
    "    c=confusion_matrix(y_test, test_predictions)\n",
    "    print('  neg  ',c[0])\n",
    "    print('  pos  ',c[1])\n",
    "\n",
    "    #Extract feature importnace\n",
    "    print('\\nTOP TEN IMPORTANT FEATURES:')\n",
    "    importances = forest.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    top_10 = indices[:20]\n",
    "    print([vectorizer.get_feature_names()[ind] for ind in top_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and test  Model on the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n"
     ]
    }
   ],
   "source": [
    "# Clean the reviews in the training set 'train' using review_cleaner function defined above\n",
    "# Here we use the original reviews without lemmatizing and stemming\n",
    "original_clean_reviews=review_cleaner(train['review'],lemmatize=False,stem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.9999 \n",
      " The validation accuracy is:  0.8216\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "  neg   [2102  446]\n",
      "  pos   [ 446 2006]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'great', 'waste', 'awful', 'excellent', 'terrible', 'best', 'boring', 'worse', 'wonderful', 'nothing', 'love', 'stupid', 'poor', 'minutes', 'even', 'perfect', 'movie', 'horrible']\n"
     ]
    }
   ],
   "source": [
    "train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train[\"sentiment\"], ngram=1, max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.824\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "  neg   [2115  433]\n",
      "  pos   [ 447 2005]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'great', 'awful', 'waste', 'excellent', 'terrible', 'wonderful', 'boring', 'nothing', 'worse', 'waste time', 'stupid', 'best', 'love', 'horrible', 'minutes', 'movie', 'even', 'plot']\n"
     ]
    }
   ],
   "source": [
    "train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train[\"sentiment\"], ngram=2, max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TODO: \n",
    "To do this exercise you only need to change argument values in the functions review_cleaner and train_predict_semtiment.\n",
    "\n",
    "1. In __UNIGRAM setting__ ie. when `ngram=1` in the function `train_predict_sentiment()`.Compare the performance of original cleaned reviews in Sentiment anlysis to \n",
    "    1. lemmatized reviews \n",
    "    2. stemmed reviews\n",
    "2. In __BIGRAM setting__ ie. when `ngram=2` in the function `train_predict_sentiment()`.Compare the performance of original cleaned reviews in sentiment analysis to:\n",
    "     1. lemmatized reviews\n",
    "     2. stemmed reviews\n",
    "3. In __UNIGRAM setting__  and `lemmatize=True` ie. when `ngram=1`, compare the performance of Sentiment analysis for these values of maximum features=[10,100,1000,5000], you can change the value of argument max_features in `train_predict_sentiment()`\n",
    "    \n",
    "### SUBMISSION:  For each question in 5. TODO  report your results in a PDF. Write a 100-200 word summary of your observations overall. Submit to Gradescope HW 9.\n",
    "### Mention the  review_cleaner( ) and train_predict_sentiment( ) argument setting that you used in each case. Do not submit any ipython notebook.\n",
    "\n",
    "\n",
    "\n",
    "Eg: For original clean reviews with unigram and 5000 max_features, I will report:\n",
    "original_clean_reviews=review_cleaner(train['review'],lemmatize=False,stem=False)  \n",
    "train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train[\"sentiment\"],ngram=1,max_features=5000)\n",
    "\n",
    "The training accuracy is:  0.9999  \n",
    "The validation accuracy is:  0.8216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
