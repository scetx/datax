{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![data-x](https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "#### NAME:\n",
    "\n",
    "#### STUDENT ID:\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Solutions] Sentiment Analysis on IMDB movie reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make compatible with Python 2 and Python 3\n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "#### About \n",
    "\n",
    "As you go through the notebook, you will encounter these main steps in the code: \n",
    "1. Reading of file `labeledTrainData.tsv` from data folder in a dataframe `train`.<br>\n",
    "2. A function `review_cleaner(train['review'],lemmatize,stem)` which cleans the reviews in the input file.<br>\n",
    "3. A function `train_predict_sentiment(cleaned_reviews, y=train[\"sentiment\"],ngram=1,max_features=1000)`<br>\n",
    "4. You will see a model has been trained on unigrams of the reviews without lemmatizing and stemming.<br>\n",
    "5. Your task is in the **To-Do** section of this notebook.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Data description\n",
    ">Data source: https://www.kaggle.com/c/word2vec-nlp-tutorial/data<br>\n",
    "\n",
    ">Data Description:<br><br>\n",
    ">We will be using Kaggle's **Bag of Words Meets Bags of Popcorn** dataset to explore [IMBD](https://www.imdb.com/) movie review data. The dataset is placed in the module folder containting this notebook if you cloned the [Data-X](https://datax.berkeley.edu/) Github repo). Labeled training dataset consists of 25,000 IMDB movie reviews. There is also an unlabeled test set with 25,000 IMDB movie reviews. The sentiment of the reviews are binary, meaning an IMDB rating < 5 results in a sentiment score of 0, and a rating >=7 have a sentiment score of 1 (no reviews with score 5 or 6 are included in the analysis). No individual movie has more than 30 reviews.\n",
    "\n",
    "\n",
    "\n",
    ">Data Sets:<br>\n",
    ">* ```labeledTrainData.tsv``` --> The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id (numerical), sentiment (categorical), and text for each review (textual).<br>\n",
    ">* ```testData.tsv``` --> The unlabeled test set. 25,000 rows containing an id (numerical), and text for each review (textual). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "___\n",
    "\n",
    "## Data Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ehch/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ehch/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ehch/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ehch/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# regular expressions, text parsing, and ML classifiers\n",
    "import re\n",
    "import nltk\n",
    "import bs4 as bs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "\n",
    "# download NLTK classifiers\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# import ml classifiers\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "from nltk.stem import PorterStemmer     # parsing/stemmer\n",
    "from nltk.tag import pos_tag            # parts-of-speech tagging\n",
    "from nltk.corpus import wordnet         # sentiment scores\n",
    "from nltk.stem import WordNetLemmatizer # stem and context\n",
    "from nltk.corpus import stopwords       # stopwords\n",
    "from nltk.util import ngrams            # ngram iterator\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "# train.shape should be (25000,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 5 rows\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "## Preparing data for classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the function `review_cleaner` to read in reviews and:\n",
    "\n",
    "> - Removes HTML tags (using beautifulsoup)\n",
    "> - Extract emoticons (emotion symbols, aka smileys :D )\n",
    "> - Removes non-letters (using regular expression)\n",
    "> - Converts all words to lowercase letters and tokenizes them (using .split() method on the review strings, so that every word in the review is an element in a list)\n",
    "> - Removes all the English stopwords from the list of movie review words\n",
    "> - Join the words back into one string seperated by space, append the emoticons to the end\n",
    "\n",
    "<br>\n",
    "\n",
    "**Pro Tip:** Transform the list of stopwords to a set before removing the stopwords -- i.e. assign `eng_stopwords = set(stopwords.words(\"english\"))`. Use the set to look up stopwords. This will substantially speed up the computations (Python is much quicker when searching a set than a list).\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_cleaner(review, lemmatize=True, stem=False):\n",
    "    '''\n",
    "        Clean and preprocess a review.\n",
    "            1. Remove HTML tags\n",
    "            2. Extract emoticons\n",
    "            3. Use regex to remove all special characters (only keep letters)\n",
    "            4. Make strings to lower case and tokenize / word split reviews\n",
    "            5. Remove English stopwords\n",
    "            6. Lemmatize\n",
    "            7. Rejoin to one string\n",
    "        \n",
    "        @review (type:str) is an unprocessed review string\n",
    "        @return (type:str) is a 6-step preprocessed review string\n",
    "    '''\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    cleaned_reviews=[]\n",
    "    for i,review in enumerate(train['review']):\n",
    "        # batching step notification\n",
    "        if( (i+1)%1000 == 0 ):\n",
    "            print(\"Done with %d reviews\" %(i+1))\n",
    "        \n",
    "        \n",
    "        #1. Remove HTML tags\n",
    "        review = bs.BeautifulSoup(review).text    \n",
    "\n",
    "        #2. Use regex to find emoticons\n",
    "        emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', review)\n",
    "\n",
    "        #3. Remove punctuation\n",
    "        review = re.sub(\"[^a-zA-Z]\", \" \",review)\n",
    "\n",
    "        #4. Tokenize into words (all lower case)\n",
    "        review = review.lower().split()\n",
    "\n",
    "        #5. Remove stopwords\n",
    "        eng_stopwords = set(stopwords.words(\"english\"))\n",
    "        \n",
    "        #6. Lemmatize \n",
    "        clean_review=[]\n",
    "        for word in review:\n",
    "            if word not in eng_stopwords:\n",
    "                if lemmatize is True:\n",
    "                    word=wnl.lemmatize(word)\n",
    "                elif stem is True:\n",
    "                    if word == 'oed':\n",
    "                        continue\n",
    "                    word=ps.stem(word)\n",
    "                clean_review.append(word)\n",
    "\n",
    "        #7. Join the review to one sentence\n",
    "        review_processed = ' '.join(clean_review+emoticons)\n",
    "        cleaned_reviews.append(review_processed)\n",
    "    \n",
    "\n",
    "    return(cleaned_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "___\n",
    "\n",
    "## Train and validate sentiment analysis model using Random Forest Classifier (RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics                          # evaluating model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#CountVectorizer can actucally handle a lot of the preprocessing for us\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict_sentiment(cleaned_reviews, y=train[\"sentiment\"], ngram=1, max_features=1000):\n",
    "    '''\n",
    "        This function will:\n",
    "            1. split data into train and test set.\n",
    "            2. get n-gram counts from cleaned reviews \n",
    "            3. train a random forest model using train n-gram counts and y (labels)\n",
    "            4. test the model on your test split\n",
    "            5. print accuracy of sentiment prediction on test and training data\n",
    "            6. print confusion matrix on test data results\n",
    "\n",
    "            To change n-gram type, set value of ngram argument\n",
    "            To change the number of features you want the countvectorizer to generate, set the value of max_features argument\n",
    "            \n",
    "            @cleaned_review (type:str) is preprocessed string from review_cleaner()\n",
    "            @return none\n",
    "    '''\n",
    "\n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer\" is scikit-learn's bag of words tool, here we show more keywords \n",
    "    vectorizer = CountVectorizer(ngram_range=(1, ngram),\n",
    "                                 analyzer = \"word\",   \n",
    "                                 tokenizer = None,    \n",
    "                                 preprocessor = None, \n",
    "                                 stop_words = None,   \n",
    "                                 max_features = max_features) \n",
    "    \n",
    "    # train / test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(cleaned_reviews, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarraty() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train).toarray()\n",
    "    test_bag = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 50 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "    # predict\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    # validation\n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    print(\" The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    print()\n",
    "    print('CONFUSION MATRIX:')\n",
    "    print('           Predicted')\n",
    "    print('            neg  pos')\n",
    "    print(' Actual')\n",
    "    c=confusion_matrix(y_test, test_predictions)\n",
    "    print('    neg  ',c[0])\n",
    "    print('    pos  ',c[1])\n",
    "\n",
    "    #Extract feature importance\n",
    "    print('\\nTOP TEN IMPORTANT FEATURES:')\n",
    "    importances = forest.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    top_10 = indices[:10]\n",
    "    print([vectorizer.get_feature_names()[ind] for ind in top_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "___\n",
    "\n",
    "## Train and test  Model on the IMDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 1000 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 25000 reviews\n"
     ]
    }
   ],
   "source": [
    "# Clean the reviews in the training set 'train' using review_cleaner function defined above\n",
    "# Here we use the original reviews without lemmatizing and stemming\n",
    "original_clean_reviews = review_cleaner(train['review'], lemmatize=False, stem=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Train RFC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.99995 \n",
      " The validation accuracy is:  0.8242\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "           Predicted\n",
      "            neg  pos\n",
      " Actual\n",
      "    neg   [2116  432]\n",
      "    pos   [ 447 2005]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'great', 'waste', 'awful', 'excellent', 'boring', 'stupid', 'terrible', 'best']\n"
     ]
    }
   ],
   "source": [
    "# Train RFC model\n",
    "train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train[\"sentiment\"], ngram=1, max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "    \n",
    "___\n",
    "    \n",
    "## **To-Do**\n",
    "\n",
    "To do this exercise you only need to change argument values in the functions review_cleaner and train_predict_semtiment.\n",
    "\n",
    "1. In **UNIGRAM setting** ie. when `ngram=1` in the function `train_predict_sentiment()`. Compare the performance of original cleaned reviews in sentiment anlysis to \n",
    "    1. lemmatized reviews \n",
    "    2. stemmed reviews\n",
    "2. In **BIGRAM setting** ie. when `ngram=2` in the function `train_predict_sentiment()`. Compare the performance of original cleaned reviews in sentiment analysis to:\n",
    "     1. lemmatized reviews\n",
    "     2. stemmed reviews\n",
    "3. In **UNIGRAM setting**  and `lemmatize=True` ie. when `ngram=1`, compare the performance of sentiment analysis for these values of maximum `features=[10,100,1000,5000]`, you can change the value of argument max_features in `train_predict_sentiment()`\n",
    "    \n",
    "\n",
    "<br>\n",
    "\n",
    "> **Eg: For original review with unigram, and `max_features=5000`:**<br><br>\n",
    "> `original_clean_reviews = review_cleaner(train['review'], lemmatize=False, stem=False)`<br>\n",
    "> `train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train['sentiment'], ngram=1, max_features=5000)`<br>\n",
    "> <br>\n",
    "> The training accuracy is:    `1.0`<br> \n",
    "> The validation accuracy is:  `0.836`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 1000 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 25000 reviews\n",
      "\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.8374\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "           Predicted\n",
      "            neg  pos\n",
      " Actual\n",
      "    neg   [2171  377]\n",
      "    pos   [ 436 2016]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'waste', 'great', 'awful', 'excellent', 'wonderful', 'worse', 'terrible', 'best']\n"
     ]
    }
   ],
   "source": [
    "# 1.A - ngram=1 and lemmatize=True\n",
    "original_clean_reviews = review_cleaner(train['review'], lemmatize=True, stem=False)\n",
    "print()\n",
    "train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train['sentiment'], ngram=1, max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 1000 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 25000 reviews\n",
      "\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.8346\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "           Predicted\n",
      "            neg  pos\n",
      " Actual\n",
      "    neg   [2156  392]\n",
      "    pos   [ 435 2017]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'wast', 'great', 'aw', 'love', 'excel', 'terribl', 'bore', 'stupid']\n"
     ]
    }
   ],
   "source": [
    "# 1.B - ngram=1 and stem=True\n",
    "original_clean_reviews = review_cleaner(train['review'], lemmatize=False, stem=True)\n",
    "print()\n",
    "train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train['sentiment'], ngram=1, max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 1000 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 25000 reviews\n",
      "\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.99995 \n",
      " The validation accuracy is:  0.8436\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "           Predicted\n",
      "            neg  pos\n",
      " Actual\n",
      "    neg   [2174  374]\n",
      "    pos   [ 408 2044]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'great', 'awful', 'excellent', 'waste', 'terrible', 'nothing', 'boring', 'waste time']\n"
     ]
    }
   ],
   "source": [
    "# 2.A - ngram=2 and lemmatize=True\n",
    "original_clean_reviews = review_cleaner(train['review'], lemmatize=True, stem=False)\n",
    "print()\n",
    "train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train['sentiment'], ngram=2, max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 1000 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 25000 reviews\n",
      "\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.8428\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "           Predicted\n",
      "            neg  pos\n",
      " Actual\n",
      "    neg   [2166  382]\n",
      "    pos   [ 404 2048]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'wast', 'great', 'aw', 'excel', 'love', 'wors', 'terribl', 'bore']\n"
     ]
    }
   ],
   "source": [
    "# 2.B - ngram=2 and stem=True\n",
    "original_clean_reviews = review_cleaner(train['review'], lemmatize=False, stem=True)\n",
    "print()\n",
    "train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train['sentiment'], ngram=2, max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 1000 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 25000 reviews\n"
     ]
    }
   ],
   "source": [
    "# 3 - ngram=1 and lemmatize=True\n",
    "original_clean_reviews = review_cleaner(train['review'], lemmatize=True, stem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MAX FEATURES =  10\n",
      "\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.8714 \n",
      " The validation accuracy is:  0.5648\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "           Predicted\n",
      "            neg  pos\n",
      " Actual\n",
      "    neg   [1421 1127]\n",
      "    pos   [1049 1403]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['film', 'movie', 'one', 'good', 'character', 'time', 'like', 'get', 'story', 'even']\n",
      "==================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "MAX FEATURES =  100\n",
      "\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.99985 \n",
      " The validation accuracy is:  0.7164\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "           Predicted\n",
      "            neg  pos\n",
      " Actual\n",
      "    neg   [1852  696]\n",
      "    pos   [ 722 1730]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'great', 'movie', 'film', 'one', 'even', 'best', 'like', 'love', 'nothing']\n",
      "==================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "MAX FEATURES =  1000\n",
      "\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.8224\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "           Predicted\n",
      "            neg  pos\n",
      " Actual\n",
      "    neg   [2116  432]\n",
      "    pos   [ 456 1996]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['worst', 'bad', 'great', 'awful', 'waste', 'boring', 'best', 'excellent', 'worse', 'terrible']\n",
      "==================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "MAX FEATURES =  5000\n",
      "\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.838\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "           Predicted\n",
      "            neg  pos\n",
      " Actual\n",
      "    neg   [2176  372]\n",
      "    pos   [ 438 2014]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'great', 'waste', 'awful', 'excellent', 'boring', 'terrible', 'wonderful', 'nothing']\n",
      "==================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = [10,100,1000,5000]\n",
    "for feat in features:\n",
    "    print(\"\\n====================================================================================================\")\n",
    "    print(\"MAX FEATURES = \", feat)\n",
    "    print()\n",
    "    train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train['sentiment'], ngram=1, max_features=feat)\n",
    "    print(\"==================================================================================================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "Please submit your the following via the instructed method (lecture or Syllabus): \n",
    "\n",
    ">(1) A copy of your work for the entirety of the **To-Do** section as a pdf, by the assignment deadline.<br>\n",
    ">(2) Write a 100-200 word summary of your observations overall. Include the argument settings you used when calling the `review_cleaner()` and `train_predict_sentiment()` functions.<br>\n",
    ">(3) Do not submit an `.ipynb` notebook/lab file.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note:** Don't gorget to restart your kernel prior to extracting your data.\n",
    "\n",
    ">```Kernel --> Restart Kernel and Run all Cells```<br>\n",
    ">```File --> Export Notebooks As --> PDF``` (or as instructed)\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
