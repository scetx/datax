{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter Grader\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![data-x](https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "#### NAME:\n",
    "\n",
    "#### STUDENT ID:\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  HW3-4: NLP (Text Processing and  Feature Engineering & Text Representation)\n",
    "**(Total 120 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP for Sentiment Analysis on IMDB Movie Reviews\n",
    "\n",
    "In this assignment we will be exploring tools for Natural Language Processing (NLP). Our task is sentiment analysis for movie reviews and in that context we will touch upon multiple areas:\n",
    "\n",
    "- Feature engineering\n",
    "- Bag of words modeling\n",
    "- Word2Vec modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to install the packages you need for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to load the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import Beautiful Soup, NumPy and Pandas, etc\n",
    "import bs4 as bs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    " \n",
    "# download NLTK classifiers - these are cached locally on your machine\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# import ml classifiers\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "from nltk.stem import PorterStemmer     # parsing/stemmer\n",
    "from nltk.tag import pos_tag            # parts-of-speech tagging\n",
    "from nltk.corpus import wordnet         # sentiment scores\n",
    "from nltk.stem import WordNetLemmatizer # stem and context\n",
    "from nltk.corpus import stopwords       # stopwords\n",
    "from nltk.util import ngrams            # ngram iterator\n",
    "\n",
    "# import word2vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize, FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "**(Total 70 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "___\n",
    "\n",
    "### Data Description\n",
    ">Data source: https://www.kaggle.com/c/word2vec-nlp-tutorial/data (originally from [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/))<br>\n",
    ">\n",
    ">Data Description:<br><br>\n",
    ">We will be using Kaggle's **Bag of Words Meets Bags of Popcorn** dataset to explore [IMBD](https://www.imdb.com/) movie review data.  Labeled training dataset consists of 25,000 IMDB movie reviews. The sentiment of the reviews are binary, meaning an IMDB rating < 5 results in a sentiment score of 0, and a rating >=7 have a sentiment score of 1 (no reviews with score 5 or 6 are included in the analysis). No individual movie has more than 30 reviews. The training data set is constructed in a balanced way so that there are an equal number of positive and negative reviews for each movie.\n",
    ">\n",
    ">Data Set:<br>\n",
    ">* ```labeledTrainData.tsv``` --> The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id (numerical), sentiment (categorical), and text for each review (textual).<br>\n",
    ">\n",
    ">\n",
    "> Further Reading:<br>\n",
    "> \n",
    "> * [Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 5 rows\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.a Clean function\n",
    "**(Total 40 points)**\n",
    "\n",
    "Finish the function `review_cleaner` to preprocess reviews. Here is an overview of what it does:\n",
    "\n",
    "> - Removes HTML tags (using beautifulsoup)\n",
    "> - Extract emoticons (emotion symbols, aka smileys :D )\n",
    "> - Removes non-letters (using regular expression)\n",
    "> - Converts all words to lowercase letters and tokenizes them (using .split() method on the review strings, so that every word in the review is an element in a list)\n",
    "> - Removes all the English stopwords from the list of movie review words\n",
    "> - Applies either stemming or lemmatization, as indicated by the arguments\n",
    "> - Join the words back into one string seperated by space, append the emoticons to the end\n",
    "\n",
    "More details can be found in the introduction [slides](https://datax.berkeley.edu/wp-content/uploads/2020/06/Module-1_-Preprocessing-Slides.pdf).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_a_0\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def review_cleaner(review, lemmatize=True, stem=False):\n",
    "    '''\n",
    "        Clean and preprocess a review.\n",
    "            1. Remove HTML tags\n",
    "            2. Extract emoticons\n",
    "            3. Use regex to remove all special characters (only keep letters)\n",
    "            4. Make strings to lower case and tokenize / word split reviews\n",
    "            5. Remove English stopwords\n",
    "            6. Lemmatize\n",
    "            7. Rejoin to one string\n",
    "        \n",
    "        @review (type:str) is an unprocessed review string\n",
    "        @return (type:str) is a 6-step preprocessed review string\n",
    "    '''\n",
    "\n",
    "    \n",
    "\n",
    "    if lemmatize == True and stem == True:\n",
    "        raise RuntimeError(\"May not pass both lemmatize and stem flags\")\n",
    "\n",
    "    #1. Remove HTML tags\n",
    "    review = ...\n",
    "\n",
    "    #2. Use regex to find emoticons\n",
    "    emoticons = ...\n",
    "\n",
    "    #3. Remove punctuation\n",
    "    review = ...\n",
    "\n",
    "    #4. Tokenize into words (all lower case)\n",
    "    review = ...\n",
    "\n",
    "    #5. Remove stopwords, Lemmatize, Stem\n",
    "    ### YOUR CODE HERE ###\n",
    "    ...\n",
    "    \n",
    "    #6. Join the review to one sentence\n",
    "    review_processed = ...\n",
    "    \n",
    "    return review_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_a_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_a_1\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for HTML Tags (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_a_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_a_2\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for emoticons (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_a_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_a_3\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for stopwords, Lemmatize, Stem (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_a_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b Set up your review\n",
    "**(Total 0 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To make things interesting, everyone gets to analyze a different review. Set `seed_value` to your favorite number, your name, or whatever else you'd like.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_b\n",
    "manual: false\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out a cleaned version of the randomly selected review\n",
    "my_review_id = int(hashlib.md5(str(seed_value).encode(\"utf-8\")).hexdigest()[:8], 16) % len(train.index)\n",
    "my_review = train.iloc[my_review_id][\"review\"]\n",
    "print(my_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c Find the stopwords\n",
    "\n",
    "**(Total 10 points)**\n",
    "\n",
    "Find the first 5 stopwords in your chosen review. \n",
    "\n",
    "First review the list of stopwords below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the stopwords are\n",
    "print(\" \".join(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "For your selected review, find the 5 first stopwords. Store them in the list named `first_5_stopwords` in the order in which they appear in the review.\n",
    "\n",
    "e.g., \n",
    "```\n",
    "first_5_stopwords = ['having', 'the', 'to', 'some', 'of']\n",
    "```\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_c\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_5_stopwords = ...\n",
    "first_5_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.d Lemmatization\n",
    "\n",
    "**(Total 10 points)**\n",
    "\n",
    "Lemmatization allows grouping of common forms of a word.\n",
    "\n",
    "Here are some examples of lemmatization:\n",
    "* images -> image\n",
    "* waxworks -> waxwork\n",
    "* sweets -> sweet\n",
    "\n",
    "Find the first 3 words in `my_review` that are lemmatized. Store them in the list named `first_3_lemmatized` in the order in which they appear in the review.\n",
    "\n",
    "E.g.:\n",
    "```\n",
    "first_3_lemmatized = ['images', 'waxworks', 'sweets']\n",
    "```\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_d\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_3_lemmatized = ...\n",
    "\n",
    "print(\"Lemmatization examples:\")\n",
    "for w in first_3_lemmatized:\n",
    "    print(\"{} -> {}\".format(w, wnl.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.e Stemming\n",
    "\n",
    "**(Total 10 points)**\n",
    "\n",
    "Stemming allows grouping of common forms of a word.\n",
    "\n",
    "Here are some examples of stemming:\n",
    "* nonsense -> nonsens\n",
    "* investigates -> investig\n",
    "* disappearance -> disappear\n",
    "\n",
    "Find the first 3 words in `my_review` that are modified by stemming. Store them in the list named `first_3_stemmed` in the order in which they appear in the review.\n",
    "\n",
    "E.g.:\n",
    "```\n",
    "first_3_stemmed = ['nonsense', 'investigates', 'disappearance']\n",
    "```\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_e\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_3_stemmed = ...\n",
    "\n",
    "print(\"Stemming examples:\")\n",
    "for w in first_3_stemmed:\n",
    "    print(\"{} -> {}\".format(w, ps.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "___\n",
    "\n",
    "## 2. Train and Validate a Sentiment Analysis Model using a Random Forest Classifier\n",
    "**(Total 30 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we have written the code to train the classifier for you. Your task will be to explore its performance characteristics with your own movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We vectorize the text using a bag of words model\n",
    "def get_vectorizer(ngram, max_features):\n",
    "    return CountVectorizer(ngram_range=(1, ngram),\n",
    "                             analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = review_cleaner,\n",
    "                             stop_words = None, \n",
    "                             max_features = max_features)\n",
    "\n",
    "# Model training\n",
    "def train_predict_sentiment(reviews, vectorizer, y=train[\"sentiment\"], ngram=1, max_features=1000, model_random_state=0):\n",
    "    '''\n",
    "        This function will:\n",
    "            1. split data into train and test set.\n",
    "            2. get n-gram counts from cleaned reviews \n",
    "            3. train a random forest model using train n-gram counts and y (labels)\n",
    "            4. test the model on your test split\n",
    "            5. print accuracy of sentiment prediction on test and training data\n",
    "            6. print confusion matrix on test data results\n",
    "\n",
    "            To change n-gram type, set value of ngram argument\n",
    "            To change the number of features you want the countvectorizer to generate, set the value of max_features argument\n",
    "            \n",
    "            @cleaned_review (type:str) is preprocessed string from review_cleaner()\n",
    "            @return none\n",
    "    '''\n",
    "\n",
    "    print(\"Creating the model!\\n\")\n",
    "    \n",
    "    # train / test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reviews, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarray() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train)\n",
    "    if not isinstance(train_bag, np.ndarray):\n",
    "        train_bag = train_bag.toarray()\n",
    "    test_bag = vectorizer.transform(X_test)\n",
    "    if not isinstance(test_bag, np.ndarray):\n",
    "        test_bag = test_bag.toarray()\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 50 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50, random_state = model_random_state) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "    # predict\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    # validation\n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    print(\" The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    print()\n",
    "    print('CONFUSION MATRIX:')\n",
    "    print('         Predicted')\n",
    "    print('          neg pos')\n",
    "    print(' Actual')\n",
    "    c=confusion_matrix(y_test, test_predictions)\n",
    "    print('     neg  ',c[0])\n",
    "    print('     pos  ',c[1])\n",
    "\n",
    "    return forest\n",
    "\n",
    "# Print out the top features\n",
    "def top_features(forest, vectorizer, n):\n",
    "    #Extract feature importance\n",
    "    print('\\nTOP TEN IMPORTANT FEATURES:')\n",
    "    feature_text = vectorizer.get_feature_names().copy()\n",
    "    feature_importance = forest.feature_importances_.copy()\n",
    "    \n",
    "    indices = np.argsort(feature_importance)[::-1]\n",
    "    \n",
    "    top_n_ind = indices[:n]\n",
    "    top_n = list([vectorizer.get_feature_names()[ind] for ind in top_n_ind])\n",
    "    \n",
    "    return top_n\n",
    "\n",
    "# Print out whether the prediction is accurate\n",
    "def check_prediction(model, vectorizer, review, expected):\n",
    "    prediction = model.predict(vectorizer.transform([review]))[0]\n",
    "    sentiment = \"üëç\" if prediction else \"üëé\"\n",
    "    correct = \"\\x1b[92mcorrect\\x1b[0m\" if prediction == expected else \"\\x1b[31mincorrect\\x1b[0m\"\n",
    "    print(\"{} ‚ü∂ {} {}\".format(review, sentiment, correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "### 2.a Train Random Forest Classifier Model\n",
    "\n",
    "**(Total 15 points)**\n",
    "\n",
    "Use the above functions to train your random forest model. Set `ngram=1`, `max_features=100` for the `get_vectorizer` function, then use  `train_predict_sentiment` function to train your model using the train dataset. Finally, use `top_features` function to print the top 10 features. This cell may take a few minutes to run.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "manual: false\n",
    "points: 15\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train RFC model\n",
    "vectorizer = ...\n",
    "forest_model = ...\n",
    "top_10 = ...\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.b Construct a positive sentiment review\n",
    "\n",
    "**(Total 5 points)**\n",
    "\n",
    "Think of a movie that you like and write a review for it. Store as a string in `good_review`. If the model doesn't give a positive prediction for your review iterate on it until it does.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_b\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_review = ...\n",
    "check_prediction(forest_model, vectorizer, good_review, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.c Construct a negative sentiment review\n",
    "\n",
    "**(Total 5 points)**\n",
    "\n",
    "Think of a movie that you like and write a review for it. Store as a string in `bad_review`. If the model doesn't give a negative prediction for your review iterate on it until it does.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_c\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_review = ...\n",
    "check_prediction(forest_model, vectorizer, bad_review, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.d - Construct a misclassified negative sentiment review\n",
    "\n",
    "**(Total 5 points)**\n",
    "\n",
    "Now try to write a review that you view as negative but the model views as positive. Iterate and experiment as necessary and store it as a string  `bad_review_error`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_d\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_review_error = ...\n",
    "check_prediction(forest_model, vectorizer, bad_review_error, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2Vec Model\n",
    "**(Total 20 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to train the Word2Vec Model for train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences=[utils.simple_preprocess(review) for review in train['review']], size=100, workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 3.a - Word2Vec similarity\n",
    "\n",
    "**(Total 5 points)**\n",
    "\n",
    "Use the Word2Vec Model we trained above to find 10 similar words for `'actors'`. Store the result in `sim`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_a\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = ...\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 3.b - Word2Vec doesn't mach\n",
    "\n",
    "**(Total 5 points)**\n",
    "\n",
    "\n",
    "Use the Word2Vec Model we trained above to find the word that doesn't match the others. We will test the words: 'professor', 'engineer', 'scientist', 'cat'. Store the result in `no_match`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_b\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_match = ...\n",
    "no_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c Fit the Word2Vec model\n",
    "\n",
    "**(Total 10 points)**\n",
    "\n",
    "Vector Averaging to get feature encoding of review:\n",
    "\n",
    "\n",
    "One challenge with the IMDB dataset is the variable-length reviews. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review.\n",
    "\n",
    "We can use vector operations to combine the words in each review. One method we tried was to simply average the word vectors in a given review (for this purpose, we removed stop words, which would just add noise).\n",
    "\n",
    "The following code averages the feature vectors. You don't need to modify the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_feature_vecs(reviews, model):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one \n",
    "\n",
    "    \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    reviewFeatureVecs = []\n",
    "    # Loop through the reviews\n",
    "    for counter, review in enumerate(reviews):\n",
    "        \n",
    "        # Print a status message every 5000th review\n",
    "        if (counter + 1) % 5000. == 0.:\n",
    "            print(\"Review %d of %d\" % (counter + 1, len(reviews)))\n",
    "\n",
    "        # Function to average all of the word vectors in a given paragraph\n",
    "        featureVec = []\n",
    "\n",
    "        # Loop over each word in the review and, if it is in the model's\n",
    "        # vocaublary, add its feature vector to the total        \n",
    "        for n,word in enumerate(utils.simple_preprocess(review)):\n",
    "            if word in index2word_set: \n",
    "                featureVec.append(model.wv[word])\n",
    "\n",
    "        \n",
    "        \n",
    "        # Average the word vectors\n",
    "        featureVec = np.mean(featureVec, axis=0).reshape(1,-1)\n",
    "\n",
    "        reviewFeatureVecs.append(featureVec)\n",
    "\n",
    "    return np.concatenate(reviewFeatureVecs, axis=0)\n",
    "\n",
    "w2v_vectorizer = FunctionTransformer(lambda x: get_avg_feature_vecs(x, w2v_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Again, use the function `train_predict_sentiment` to train your random forest model. Remember: we vectorize the text using the Word2Vec model now. Save the trained model in `w2v_forest_model`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_c\n",
    "manual: false\n",
    "points: 10\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2v_forest_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Word2Vec compares with the Bag of Words Model? Is it an improvement? How significant is the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.d Word2Vec Prediction Analysis\n",
    "\n",
    "**(Total 0 points)**\n",
    "\n",
    "Run the following cells to check to see how the Word2Vec model works on the reviews that you wrote previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_prediction(w2v_forest_model, w2v_vectorizer, good_review, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_prediction(w2v_forest_model, w2v_vectorizer, bad_review, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With Bag of Words:\")\n",
    "check_prediction(forest_model, vectorizer, bad_review_error, 0)\n",
    "\n",
    "print(\"With Word2Vec:\")\n",
    "check_prediction(w2v_forest_model, w2v_vectorizer, bad_review_error, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the questions below:\n",
    "\n",
    "* Is your positive review classified correctly by Word2Vec?\n",
    "\n",
    "* Is your negative review classified correctly by Word2Vec?\n",
    "\n",
    "* Is your negative review misclassified by Bag of Words now classified correctly by Word2Vec?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submit\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output.\n",
    "**Please save before submitting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to create a pdf for your reference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
