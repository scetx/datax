{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter Grader\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMpNvKD45jL4"
   },
   "source": [
    "![data-x](https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png)\n",
    "\n",
    "___\n",
    "\n",
    "#### NAME:\n",
    "\n",
    "#### STUDENT ID:\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlsCVU1m5odo"
   },
   "source": [
    "# **HW3-4: Convolutional Neural Networks**\n",
    "**(Total 120 points)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8VPyU0jVFqy"
   },
   "source": [
    "In this homework, you will compare the performance achieved by convolutional neural networks (CNNs) with the fully connected networks and also some shallow learning methods such as SVM in classifying the images from the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeKZYD8d7ZO5"
   },
   "source": [
    "## 1. Loading and Exploring the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqgvSkIYWDdE"
   },
   "source": [
    "Run the following cell to load the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ID0F8hG49pxt"
   },
   "outputs": [],
   "source": [
    "## Load the required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCAFEMSi7D6W"
   },
   "source": [
    "CIFAR is an acronym that stands for the Canadian Institute For Advanced Research. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. This dataset was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Read more about this dataset [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "The class labels and their standard associated integer values are listed below:\n",
    "\n",
    "* 0: airplane\n",
    "* 1: car\n",
    "* 2: bird\n",
    "* 3: cat\n",
    "* 4: deer\n",
    "* 5: dog\n",
    "* 6: frog\n",
    "* 7: horse\n",
    "* 8: ship\n",
    "* 9: truck\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcJXj6H8W_Be"
   },
   "source": [
    "Run the following cell without any modifications to load the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a04WenMF99Pu"
   },
   "outputs": [],
   "source": [
    "## Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zLyoOGeWep3"
   },
   "source": [
    "Let's make sure that the number and shape of the training and test images are as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9P7gwQl-_-f",
    "outputId": "f1b32595-21c7-4bba-fefa-4188d9f8c671"
   },
   "outputs": [],
   "source": [
    "## Run this cell, no need to modify\n",
    "print('Training: x_train=%s, y_train=%s' % (x_train.shape, y_train.shape))\n",
    "print('Test: x_test=%s, y_test=%s' % (x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXKAU4OXXf-F"
   },
   "source": [
    "The next cell plots the first 16 images from this dataset. It is clear that the images are indeed very small compared to modern photographs; it can be challenging to see what exactly is represented in some of the images given the extremely low resolution. Check the 11th image for example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "klxy675DCwe8",
    "outputId": "3879973b-2956-4f57-ba28-9ae773aa5de8"
   },
   "outputs": [],
   "source": [
    "# Run this cell, no need to modify\n",
    "int2label = {0: 'airplane', 1: 'car', 2: 'bird', 3: 'cat', \n",
    "             4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', \n",
    "             9: 'truck'}\n",
    "fig, axs = plt.subplots(4, 4, figsize=(10, 10))\n",
    "axs = axs.flatten()\n",
    "for i, ax in enumerate(axs):\n",
    "  ax.imshow(x_train[i])\n",
    "  ax.set_title('This is a %s' % int2label[y_train[i].item()])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq92kO47FEYd"
   },
   "source": [
    "You might think that this low resolution is likely to limit the performance achieved by machine learning algorithms, but you should not underestimate the power of deep learning. Checkout this [leaderbord](https://paperswithcode.com/sota/image-classification-on-cifar-10) to see the performance that top-of-the-line deep learning algorithms are able to achieve on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFNxPbrz8k71"
   },
   "source": [
    "## 2. Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZsaXGnZaSvD"
   },
   "source": [
    "For the purpose of this homework, we pick the first 49000 training images for as training set and the last 1000 training images as the validation set. We do not touch the test set until the last part of this homework. \n",
    "\n",
    "Run the following cell to get the training, validation, and test sets and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1JNIsEOLrnn",
    "outputId": "6381763f-38a8-43aa-b17a-de20a9ac104e"
   },
   "outputs": [],
   "source": [
    "## Run this cell, no need to modify\n",
    "\n",
    "# Valdiation set\n",
    "x_val = x_train[49000:]\n",
    "y_val = np.squeeze(y_train[49000:])\n",
    "\n",
    "# Training set\n",
    "x_train = x_train[:49000]\n",
    "y_train = np.squeeze(y_train[:49000])\n",
    "\n",
    "# Test set\n",
    "x_test = x_test\n",
    "y_test = np.squeeze(y_test)\n",
    "\n",
    "print('Training: x_train=%s, y_train=%s' % (x_train.shape, y_train.shape))\n",
    "print('Validation: x_val=%s, y_val=%s' % (x_val.shape, y_val.shape))\n",
    "print('Test: x_test=%s, y_test=%s' % (x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmtVkuTkfPQ3"
   },
   "source": [
    "The pixel values for each image in the dataset are unsigned integers in the range between no color and full color, or 0 and 255. Thus, we need to convert the data type from unsigned integers to floats.\n",
    "\n",
    "Furthermore, neural networks process inputs using small weight values, and inputs with large integer values can disrupt or slow down the learning process. As such, it is a good practice to normalize the pixel values so that each pixel value has a value between 0 and 1. Dividing the pixel values by the maximum value does the job.\n",
    "\n",
    "Running the following cell changes the data type of each pixel and normalize their value. We store the pixels as 16-bit (half-precision) float to save the memory without sacrificing much accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0e8T7C6OdAIX"
   },
   "outputs": [],
   "source": [
    "# Run this cell, no need to modify\n",
    "x_train = x_train.astype('float16') / 255.0\n",
    "x_val = x_val.astype('float16') / 255.0\n",
    "x_test = x_test.astype('float16') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeTrN3jaOoqU"
   },
   "source": [
    "##  3. Shallow Learning on CIFAR-10\n",
    "\n",
    "**(Total 40 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9t7MxK3kQlO"
   },
   "source": [
    "Our ultimate goal is to have a model achieving a high accuracy on the **validation set** (why not the test set?). First, let's see how some of the models you learnt in the previous homework (core concepts) perform on this dataset. In case they achieve a high validation accuracy, then there is no need to bother ourselves with the neural nets.\n",
    "\n",
    "Run the following cell to load the required modules.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3823X7CnRCou"
   },
   "outputs": [],
   "source": [
    "## Load the required modules\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftecL5TZrfDJ"
   },
   "source": [
    "Since the models you havve seen in the HW2 do not process images and rather use a vector of features, we first need to flatten the images in our training and valdiation sets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "W7l_HSPmrnjk"
   },
   "source": [
    "**3.1) (5 points)** Flatten the images in the training and validation sets and store the results in `x_train_flat` and `x_val_flat`. \n",
    "\n",
    "> **Note:** The shape of `x_train_flat` and `x_val_flat` should be (49000, 3072) and (1000, 3072), respectively.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q31\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Aa1wztZqsrhW"
   },
   "outputs": [],
   "source": [
    "## Your code here\n",
    "x_train_flat = ...\n",
    "x_val_flat = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf7mxK_7mdSb"
   },
   "source": [
    "\n",
    "Currently, we have 49000 training data points each with 3072 features. In order to be able to run some of the algorithms in HW2 on this dataset in a reasonable amount of time, we need to reduce the dimensionality of the features. \n",
    "\n",
    "There are plenty of ways to reduce the dimensionality of the problem, but here we use PCA. Do not worry if you are not familiar with this method as we have implemented it for you. If you are curious to know how PCA works, check [this](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=Principal%20component%20analysis%20(PCA)%20is,components%20and%20ignoring%20the%20rest.) out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "0w967dc8yZVI",
    "outputId": "a7572d2c-1321-4634-f859-d411214c4cba"
   },
   "outputs": [],
   "source": [
    "## Run this cell, no need to modify\n",
    "combined = np.vstack((x_train_flat, x_val_flat))\n",
    "pca = PCA().fit(combined)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nOHHYT_zoNM"
   },
   "source": [
    "The plot that you observe above depicts how much variance in the data is retained for different choices of the number of principal components. For example, by reducing the number of components to 50 (from 3072), we retain 84.3% of the variance in the data. Run the following cell for different values of `n` to further explore the plot above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LGq_2lDvyPCT",
    "outputId": "13a1aea7-ce5c-47ad-80a3-602fc0480c4b"
   },
   "outputs": [],
   "source": [
    "## Run this cell for different values of n\n",
    "n = 50\n",
    "print('By reducing the number of components to %d, \\\n",
    "we retain %s percent of the variance in the data.' \\\n",
    "% (n, np.round(100 * pca.explained_variance_ratio_.cumsum()[n-1], 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIDPZH08yRid"
   },
   "source": [
    "Choose how many principal components you want to keep and store it in the variable `your_n`. Then, run the following cell to get the low dimensional training and validation sets (`x_train_reduced` and `x_val_reduced`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "X_AbzytT5Ld4"
   },
   "outputs": [],
   "source": [
    "## Your code here\n",
    "your_n = ...\n",
    "\n",
    "## Do not modify the following lines\n",
    "pca = PCA(n_components=your_n) \n",
    "pca.fit(combined)\n",
    "transformed = pca.transform(combined)\n",
    "x_train_reduced = transformed[:49000]\n",
    "x_val_reduced = transformed[49000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nwb8IZjt7SVb"
   },
   "source": [
    "**3.2) (35 points)** Train a kernel SVM using the low dimensional training set that achieves a reasonable training and validation accuracy. You will receive credit according to the following scheme:\n",
    "\n",
    " > Full credit if **99% $<$ training accuracy** and  **50% $<$ validation accuracy**.\n",
    "\n",
    " > 15 points if **99% $<$ training accuracy** and **45% $<$ validation accuracy $\\leq$ 50%**.\n",
    "\n",
    " > 0 points otherwise. \n",
    "\n",
    "**Make sure you follow these instructions:**\n",
    "\n",
    "* You should already be familiar with sklearn function [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) from part 5 of HW2. You need to set the following options/hyperparameters in SVC: `C`, `kernel`, `degree`, `gamma`, and `max_iter`.\n",
    "\n",
    "* The variables `x_train_3` and `y_train_3` contain the training set and its labels, respectively. And `x_val_3`, and `y_val_3` contain the validation set and its labels, respectively. \n",
    "\n",
    " * Although the number of features is significantly reduced (depending on your choice of the number of principal components), it could still take quite a bit of time to train the SVM model on the whole training set. You may want to choose a subset of your training data and their corresponding labels and assign them to `x_train_3` and `y_train_3`, respectively. Set `n_train` to choose the first `n_train` training data points.\n",
    "   > For example, you may choose to set `n_train = 1000` in which case you are using the first 1000 training data points and their corresponding labels to train your model.\n",
    "\n",
    " * You must not modify neither of `x_val_3` nor `y_val_3`.   \n",
    "\n",
    "* Note that the number of components you chose to keep in the previous cell, `your_n`, is important for both the training time and the validation accuracy of your model.\n",
    "\n",
    "* The autograder will be timed out after 30 minutes. Adjust the running time of your code (the entire notebook) accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OVtUbioev6Q8"
   },
   "source": [
    "The test \"q32a\" checks if you acheive >99% training accuracy and >45% validation accuracy.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q32a\n",
    "manual: false\n",
    "points: 15\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fIwFCUqx8Fv2",
    "outputId": "817d715e-31fa-46fa-bd0a-3e4013126b93"
   },
   "outputs": [],
   "source": [
    "## Your code here\n",
    "...\n",
    "\n",
    "## Do not modify the following lines\n",
    "x_train_3 = x_train_reduced[:n_train] \n",
    "y_train_3 = y_train[:n_train] \n",
    "x_val_3 = x_val_reduced\n",
    "y_val_3 = y_val\n",
    "\n",
    "time_start = timer()\n",
    "svm_model = SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, max_iter=max_iter)\n",
    "svm_model.fit(x_train_3, y_train_3)\n",
    "time_end = timer()\n",
    "print (\"Wall time for training the model: {0} second\".format(time_end-time_start))\n",
    "\n",
    "time_start = timer()\n",
    "train_acc = np.mean(svm_model.predict(x_train_3) == y_train_3)\n",
    "print('Training Accuracy = {0:f}'.format(train_acc))\n",
    "val_acc = np.mean(svm_model.predict(x_val_3) == y_val_3)\n",
    "print('Validation Accuracy = {0:f}'.format(val_acc))\n",
    "time_end = timer()\n",
    "print (\"Wall time for computing the training and validation accuracies: {0} second\".format(time_end-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q32a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8idpArZ21ldq"
   },
   "source": [
    "The test \"q32b\" checks if you acheive >99% training accuracy and >50% validation accuracy.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q32b\n",
    "manual: false\n",
    "points: 20\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "eTXyZ66i136K"
   },
   "outputs": [],
   "source": [
    "## Dummy Cell, DO NOT MODIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q32b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRs-weiCW0oo"
   },
   "source": [
    "Note that a random classifier would achieve an accuracy of about 10% on the CIFAR-10 dataset. The 50% accuracy (although it's the validation accuracy, not the test accuracy) we achieved using SVM is a big step up from a random classifier, but still it is far from being ideal. \n",
    "\n",
    "You may also pick another model from HW2, train it the way we trained the SVM model above, and see if you could achieve a higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omZ9sH58enMZ"
   },
   "source": [
    "## 4. Fully Connected Neural Networks on CIFAR-10\n",
    "**(Total 40 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj6URMhTJaC4"
   },
   "source": [
    "In this part, you will train a fully connected neural network on CIFAR-10 dataset with the hope of getting a higher validation accuracy than the one you obtained using an SVM model.\n",
    "\n",
    "Run the cell below to load the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "wXdDDIHfJbKu"
   },
   "outputs": [],
   "source": [
    "## Load the required modules\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wB1SEMmSa4ap"
   },
   "source": [
    "\n",
    "\n",
    "First of all, make sure you study the neural networks [module](https://datax.berkeley.edu/wp-content/uploads/2020/09/NN-.pdf) in the course website and go through the corresponding jupyter notebook to understand how neural nets, specifically fully connected neural nets, work. To exercise your knowledge, you may either work on the neural nets homework on the course webpage, or check the [NeuralNet](https://github.com/scetx/datax/tree/master/dataxHWSp2021/HW3-4_NeuralNet/student) homework that we have recently released.\n",
    "\n",
    "Before you start building and training your own model, as a demo, let's train a simple fully connected neural net with only one hidden layer on the CIFAR-10 dataset. \n",
    "\n",
    "Like the SVM model, we need to flatten the images before feeding them into the fully connected neural network. Note that this throws away the information about the 2D structure of the image. Also, since we will be using the [categorical_crossentropy](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class) as the model's loss function, we need to encode different image classes using [one-hot](https://en.wikipedia.org/wiki/One-hot) encoding. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DSqi5HpPNrmK"
   },
   "source": [
    "**4.1) (5 points)** Store the training and validation sets and their labels in the variables `x_train_4`, `y_train_4`, `x_val_4`, and `y_val_4`. Note that we have already flattened the images in the previous part. Use keras function [to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) to encode your training and validation labels. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q41\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "AIqg5fCkLfOv"
   },
   "outputs": [],
   "source": [
    "## Your code here\n",
    "\n",
    "# Training set\n",
    "x_train_4 = ...\n",
    "y_train_4 = ...\n",
    "\n",
    "# Validation set\n",
    "x_val_4 = ...\n",
    "y_val_4 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q41\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUuftawfQXLM"
   },
   "source": [
    "We start by creating a sequential model using keras [Sequential](https://keras.io/guides/sequential_model/) class. This allows us to build neural nets like legos, by adding one layer on top of the other, and swapping layers. Note that a sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. Such model is not appropriate when:\n",
    "\n",
    "*  Your model has multiple inputs or multiple outputs.\n",
    "*  Any of your layers has multiple inputs or multiple outputs.\n",
    "*  Your model requires layer sharing.\n",
    "*  You want to model non-linear topology (e.g. a residual connection, a multi-branch model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "_wUvRfEhfBep"
   },
   "outputs": [],
   "source": [
    "## Run this cell, no need to modify\n",
    "fc_demo = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQ-c9WDCRxoY"
   },
   "source": [
    "Now, we can import layer classes and stack layers by using `fc_demo.add()`. Model needs to know what input shape it should expect. For this reason, the first layer in a sequential model needs to receive information about its input shape. \n",
    "\n",
    "We let the hidden layer have 1000 neurons and activate them by relu function. To know more about the syntax, consult with the [Dense layer](https://keras.io/api/layers/core_layers/dense/) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Up7WoYk5RFRu"
   },
   "outputs": [],
   "source": [
    "## Run this cell, no need to modify\n",
    "fc_demo.add(Dense(units=1000, activation='relu', \\\n",
    "                 input_shape=(3072, ), name='hidden'))\n",
    "fc_demo.add(Dense(units=10, activation='softmax', name='output'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxwdSWLqSY70"
   },
   "source": [
    "Let's review the summary of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHhP0vxhRDmr",
    "outputId": "c19cb315-74aa-470d-afa2-58afa4a8633c"
   },
   "outputs": [],
   "source": [
    "## Run this cell, no need to modify\n",
    "fc_demo.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QA4IcIRUVE9"
   },
   "source": [
    "As we mentioned at the beginning of this homework, the images in CIFAR-10 dataset are indeed very small compared to modern photographs, yet as you can see, in a simple fully connected neural network with only one hidden layer, we have about 3 million trainable paramteres. You can imagine how this number would grow if we wanted to work with high rsolution images and have a network consisting of several hidden layers. \n",
    "\n",
    "Before training a model, you need to configure the learning process, which is done via the [compile](https://keras.io/api/models/model_training_apis/) method `.compile()`. `.compile` receives at least the following three arguments:\n",
    "\n",
    "* 1) A [loss](https://keras.io/api/losses/) function - This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as `categorical_crossentropy` or `mse`), or it can be an objective function.\n",
    "* 2) An [optimizer](https://keras.io/api/optimizers/) - This could be the string identifier of an existing optimizer (such as `rmsprop`, `gradientdescent`, or `adam`), or an instance of the Optimizer class.\n",
    "* 3) A list of [metrics](https://keras.io/api/metrics/) (optional) -  For any classification problem you will want to set this to `metrics=['accuracy']`. A metric could be the string identifier of an existing metric or a custom metric function.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lxt3M248XQwt"
   },
   "source": [
    "We use the `categorical_crossentropy` as our loss function, and [SGD](https://keras.io/api/optimizers/sgd/) as our optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "GG8ZPOi4hI-i"
   },
   "outputs": [],
   "source": [
    "## Run this cell, no need to modify\n",
    "fc_demo.compile(loss='categorical_crossentropy',\n",
    "             optimizer=keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95_DnbwDYlOh"
   },
   "source": [
    "Now that we have created our model and configured its learning process, we need to use the [fit](https://keras.io/api/models/model_training_apis/) method `.fit()` to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVem_XXXhKjb",
    "outputId": "b8df8c83-f850-41fc-c25f-f122ce2ba6ad"
   },
   "outputs": [],
   "source": [
    "## Run this cell, no need to modify\n",
    "fc_demo.fit(x=x_train_4, \n",
    "          y=y_train_4, \n",
    "          epochs=2, \n",
    "          validation_data=(x_val_4, y_val_4),\n",
    "          batch_size=32)\n",
    "\n",
    "_, train_acc = fc_demo.evaluate(x_train_4, y_train_4, verbose=0)\n",
    "print('Training Accuracy = {0:f}'.format(train_acc))\n",
    "_, val_acc = fc_demo.evaluate(x_val_4, y_val_4, verbose=0)\n",
    "print('Validation Accuracy = {0:f}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0sN-hDXatSC"
   },
   "source": [
    "The training and validation accuracies are relatively low. Some potential reasons could be our training process (the optimizer is not tuned, or the number of epochs is small) or the simple structure of our network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIoAoeLAd6L2"
   },
   "source": [
    "**4.2) (35 points)** Train your own fully connected neural network that achieves a reasonable training and validation accuracy. You will receive credit according to the following scheme:\n",
    "\n",
    " > Full credit if **50% $<$ training accuracy** and  **50% $<$ validation accuracy**.\n",
    "\n",
    " > 15 points if **50% $<$ training accuracy** and **45% $<$ validation accuracy $\\leq$ 50%**.\n",
    "\n",
    " > 0 points otherwise. \n",
    "\n",
    "**Make sure you follow these instructions:**\n",
    "\n",
    "* You must use `x_train_4`, `y_train_4`, `x_val_4`, and `y_val_4` to train and validate your model. You must not modify any of them. So, you will be training your model on the whole training set as we did with our demo model. \n",
    "\n",
    "* A sequential model named `fc_model` is created below. As shown above, you need to add your desired layers to this model using the add method `.add()`. \n",
    "\n",
    "* You can only use dense layers, batchnormalization layers, and dropout layers to build your model. You may want to leverage [Batchnormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization?version=nightly) to accelerate the training process and [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) to get a higher validation accuracy.\n",
    "\n",
    " > If you are not familiar with these techniques, check the [NeuralNet](https://github.com/scetx/datax/tree/master/dataxHWSp2021/HW3-4_NeuralNet/student) homework that we have recently released.\n",
    "\n",
    "* The way you [initialize](https://keras.io/api/layers/initializers/) the layer weights is also important in the learning process.\n",
    "\n",
    "* You need to choose your own optimizer `fc_optimizer` as well the number of training epochs `n_epochs` and the batch size `batch_size`.\n",
    "\n",
    "* The autograder will be timed out after 30 minutes. Adjust the running time of your code (the entire notebook) accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "iCgkfgvCmZ14"
   },
   "source": [
    "The test \"q42a\" checks if you achieve >50% training accuracy and >45% validation accuracy.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q42a\n",
    "manual: false\n",
    "points: 15\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "iVAPXLq_koj9",
    "outputId": "6d10925e-fef5-4210-e40f-ea0cefac81de"
   },
   "outputs": [],
   "source": [
    "fc_model = Sequential()\n",
    "\n",
    "## Your code here\n",
    "fc_optimizer = ...\n",
    "n_epochs = ...\n",
    "batch_size = ...\n",
    "\n",
    "## Do not modify the following cells\n",
    "fc_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=fc_optimizer,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history = fc_model.fit(x=x_train_4, \n",
    "          y=y_train_4, \n",
    "          epochs=n_epochs, \n",
    "          validation_data=(x_val_4, y_val_4),\n",
    "          batch_size=batch_size)\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(5, 5))\n",
    "# plot loss\n",
    "ax1.set_title('Cross Entropy Loss')\n",
    "ax1.plot(np.arange(n_epochs) + 1, history.history['loss'], color='blue', label='training')\n",
    "ax1.plot(np.arange(n_epochs) + 1, history.history['val_loss'], color='orange', label='validation')\n",
    "ax1.legend()\n",
    "# plot accuracy\n",
    "ax2.set_title('Classification Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.plot(np.arange(n_epochs) + 1, history.history['accuracy'], color='blue', label='training')\n",
    "ax2.plot(np.arange(n_epochs) + 1, history.history['val_accuracy'], color='orange', label='validation')\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q42a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1OjOfmwemro-"
   },
   "source": [
    "The test \"q42b\" checks if you acheive >50% training accuracy and >50% validation accuracy.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q42b\n",
    "manual: false\n",
    "points: 20\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "k8LkLzjonJdL"
   },
   "outputs": [],
   "source": [
    "## Dummy Cell, DO NOT MODIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q42b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blgJGkTtl1T9"
   },
   "source": [
    "Compare the performance of `fc_model` and `svm_model`. Which one of these models would achieve a higher test accuracy? (You don't need to write any answers.) We will compare their performance on the test set in the last part of this homework. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m16ugU7il-rw"
   },
   "source": [
    "## 5. Convolutional Neural Networks on CIFAR-10\n",
    "\n",
    "**(total 40 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X04U_b71959z"
   },
   "source": [
    "With the limitations that shallow learning methods have and the fact that fully connected neural networks are not appropriate to process images (huge number of parameters, lack of the ability to exploit the spatial information in the images, etc.), convolutional neural networks are the to-go model for most of the tasks in computer vision. In this part, you will train a convolutional neural network that achieves a relatively high validation accuracy on the CIFAR-10 dataset.\n",
    "\n",
    "Run the following cell to load the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "pMZUQv4gAcNk"
   },
   "outputs": [],
   "source": [
    "## Load the required modules \n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import AveragePooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ivPGxxQCvN5"
   },
   "source": [
    "Check the convolutional neural networks [module](https://datax.berkeley.edu/wp-content/uploads/2020/09/slides-m430-convolutional-neural-networks.pdf) to understand how conv nets work. To get more familiar with this architecture and pick up some coding skills you may take a look at the two corresponding jupyter notebooks as well. \n",
    "\n",
    "Since conv nets use the 2D/3D structure of the image, we do not need to flatten the images before feeding them into these networks. Moreover, due to the relatively small number of trainable parameters in each filter, conv nets allow us to go much deeper (in terms of the number of layers) than the fully connected networks.\n",
    "\n",
    "The training and validation sets and their corresponding labels are given to you below. Note that we again need to encode different image classes using [one-hot](https://en.wikipedia.org/wiki/One-hot) encoding. \n",
    "> Make sure you pass the test \"q41\" before running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "7rNdElbumLa-"
   },
   "outputs": [],
   "source": [
    "## Run this cell, no need to modify \n",
    "\n",
    "# Training set\n",
    "x_train_5 = x_train\n",
    "y_train_5 = y_train_4\n",
    "\n",
    "# Validation set\n",
    "x_val_5 = x_val \n",
    "y_val_5 = y_val_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNQY6UY0LAx1"
   },
   "source": [
    "As a demo, we build and train a simple conv net on our dataset. Same as the previous part, we start by creating a sequential model and then use the add method to import different layers into it and stack them on top of each other. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "a6KFhXO-LjWn"
   },
   "outputs": [],
   "source": [
    "## Run this cell, no need to modify\n",
    "cnn_demo = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLyFw14GL45n"
   },
   "source": [
    "Convolutional and pooling layers are the main building blocks of a conv net. We implement convolutional layers using  [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/). \n",
    "> Make sure you understand the concepts of stride and padding. \n",
    "\n",
    "Pooling layers are used to subsample the input image to reduce computational load, memory usage, and number of prameters. Pooling layers also require size, stride and padding type, but unlike convolutional layers, neurons in pooling layers do not have weights. We use [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/) and [AveragePooling2d](https://keras.io/api/layers/pooling_layers/average_pooling2d/) to implement these layers. \n",
    "\n",
    "After using several blocks of convolutional and pooling layers, we get something small enough that we can flatten and feed into a standard fully connected layer. \n",
    "\n",
    "The architecture that we have chosen for `cnn_demo` is quite similar to LeNet-5, the architecture that Yann LeCun, Leon Bottou, Yosuha Bengio and Patrick Haffner proposed for handwritten character recognition in 1990â€™s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "LIouhvhnJVMB"
   },
   "outputs": [],
   "source": [
    "# Run this cell, no need to modify\n",
    "cnn_demo.add(Conv2D(filters=6, kernel_size=(3, 3), padding='same', \\\n",
    "                        activation='relu', input_shape=(32,32,3))) # First layer\n",
    "cnn_demo.add(AveragePooling2D()) # Second layer\n",
    "cnn_demo.add(Conv2D(filters=16, kernel_size=(5, 5), \\\n",
    "                           activation='relu')) # Third layer\n",
    "cnn_demo.add(AveragePooling2D()) # Fourth layer\n",
    "cnn_demo.add(Flatten()) \n",
    "cnn_demo.add(Dense(units=100, activation='relu')) # Fifth layer\n",
    "cnn_demo.add(Dense(units=10, activation = 'softmax')) # Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWPDp534RJmJ"
   },
   "source": [
    "Let's review the summary of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDfxODEwmqJg",
    "outputId": "25ad44cf-4c0d-44cf-a4d1-6f8dc4828dd6"
   },
   "outputs": [],
   "source": [
    "cnn_demo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEDBzNPuR2Ro"
   },
   "source": [
    "Note the difference between the number of trainable parameters in the network above and the simple fully connected network (`fc_demo`) we built in the previous part. Even though `fc_demo` has only one hidden layer, it has considerably more parameters than the 6-layer conv net above.  \n",
    "\n",
    "Now, we need to configure the learning process and then train our model using the compile and fit methods, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Ek5BdnGmrdO",
    "outputId": "6ec99afa-6735-4a9b-a3ba-497aaab0a242"
   },
   "outputs": [],
   "source": [
    "# Run this cell, no need to modify\n",
    "cnn_demo.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = keras.optimizers.SGD(lr=0.001, momentum = 0.9, nesterov=True),\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "cnn_demo.fit(x=x_train_5, \n",
    "          y=y_train_5, \n",
    "          epochs=2, \n",
    "          validation_data=(x_val_5, y_val_5),\n",
    "          batch_size=32)\n",
    "\n",
    "_, train_acc = cnn_demo.evaluate(x_train_5, y_train_5, verbose=0)\n",
    "print('Training Accuracy = {0:f}'.format(train_acc))\n",
    "_, val_acc = cnn_demo.evaluate(x_val_5, y_val_5, verbose=0)\n",
    "print('Validation Accuracy = {0:f}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecTVpNVIUGmQ"
   },
   "source": [
    "Like the previous demo, the training and validation accuracies are relatively low. You need to try different architectures and fine-tune some of the hyperparameters to get outstanding results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Zt1gBxtUlMj"
   },
   "source": [
    "**5.1) (40 points)** Train your own convolutional neural network that achieves a high training and validation accuracy. You will receive credit according to the following scheme:\n",
    "\n",
    " > Full credit if **70% $<$ training accuracy** and  **69% $<$ validation accuracy**.\n",
    "\n",
    " > 15 points if **70% $<$ training accuracy** and **65% $<$ validation accuracy $\\leq$ 69%**.\n",
    "\n",
    " > 0 points otherwise. \n",
    "\n",
    "**Make sure you follow these instructions:**\n",
    "\n",
    "* You must use `x_train_5`, `y_train_5`, `x_val_5`, and `y_val_5` to train and validate your model. You must not modify any of them. So, you will be training your model on the whole training set as we did with our demo model. \n",
    "\n",
    "* A sequential model named `cnn_model` is created below. You need to add your desired layers to this model using the add method `.add()`. \n",
    "\n",
    "* You can only use the following layers: Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense, BatchNormalization, and Dropout. \n",
    "\n",
    "* The way you [initialize](https://keras.io/api/layers/initializers/) the layer weights is also important in the learning process.\n",
    "\n",
    "* You need to choose your own optimizer `cnn_optimizer` as well the number of training epochs `n_epochs` and the batch size `batch_size`.\n",
    "\n",
    "* The autograder will be timed out after 30 minutes. Adjust the running time of your code (the entire notebook) accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-GTqa-0tV3ys"
   },
   "source": [
    "The test \"q51a\" checks if you acheive >70% training accuracy and >65% validation accuracy.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q51a\n",
    "manual: false\n",
    "points: 15\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "k-nvMXXlV3yt",
    "outputId": "30bd7b72-cf40-4d83-8a19-3ae843ab5602"
   },
   "outputs": [],
   "source": [
    "cnn_model = Sequential()\n",
    "\n",
    "## Your code here\n",
    "cnn_optimizer = ...\n",
    "n_epochs = ...\n",
    "batch_size = ...\n",
    "\n",
    "## Do not modify the following cells\n",
    "cnn_model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=cnn_optimizer,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history = cnn_model.fit(x=x_train_5, \n",
    "          y=y_train_5, \n",
    "          epochs=n_epochs, \n",
    "          validation_data=(x_val_5, y_val_5),\n",
    "          batch_size=batch_size)\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(5, 5))\n",
    "# plot loss\n",
    "ax1.set_title('Cross Entropy Loss')\n",
    "ax1.plot(np.arange(n_epochs) + 1, history.history['loss'], color='blue', label='training')\n",
    "ax1.plot(np.arange(n_epochs) + 1, history.history['val_loss'], color='orange', label='validation')\n",
    "ax1.legend()\n",
    "# plot accuracy\n",
    "ax2.set_title('Classification Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.plot(np.arange(n_epochs) + 1, history.history['accuracy'], color='blue', label='training')\n",
    "ax2.plot(np.arange(n_epochs) + 1, history.history['val_accuracy'], color='orange', label='validation')\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q51a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "VUQVaed4V3yw"
   },
   "source": [
    "The test \"q51b\" checks if you acheive >70% training accuracy and >69% validation accuracy.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q51b\n",
    "manual: false\n",
    "points: 25\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "on3-y_jLV3yw"
   },
   "outputs": [],
   "source": [
    "## Dummy Cell, DO NOT MODIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q51b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7m3Rj9v1ZF19"
   },
   "source": [
    "## 6. Test Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1CAGJJVZOwt"
   },
   "source": [
    "At the beginning of this homework, we put a test set aside as we would like to test our models' performance on a totally unseen dataset. As we fine-tune the hyper paramteres in our models and change their structure to achieve a higher validation accuracy, the models gain access to some information from the validation set, and therefore it should not be interpreted as an unseen dataset at all.\n",
    "\n",
    "Run the cell below to find out the accuracy of the different models you trained above on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMg56sPoZOFB",
    "outputId": "3a498835-61a5-46c3-cd11-0b062395a2a9"
   },
   "outputs": [],
   "source": [
    "## Run this cell, no need to modify\n",
    "svm_test_acc = np.mean(svm_model.predict(pca.transform(x_test.reshape((x_test.shape[0], -1)))) == y_test)\n",
    "print('Test accuracy of the SVM model= {0:f}'.format(svm_test_acc))\n",
    "\n",
    "fc_test_acc = fc_model.evaluate(x_test.reshape((x_test.shape[0], -1)), to_categorical(y_test), verbose=0)[1]\n",
    "print('Test accuracy of the fully connected neural network= {0:f}'.format(fc_test_acc))\n",
    "\n",
    "cnn_test_acc = cnn_model.evaluate(x_test, to_categorical(y_test), verbose=0)[1]\n",
    "print('Test accuracy of the convolutional neural network= {0:f}'.format(cnn_test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvJuvR7zhzll"
   },
   "source": [
    "<font color='Blue'> *Important: Note that the models you initialize and train are perhaps not deterministic, unless you set all the random states (for the layers and also the optimizers). Consequently, it could be possible that you pass the tests in one run, but fail some of them in another run. The same goes with the autograder, that is you might need to submit a few times till you get all the points. So plan in advance and don't wait till the last minute to submit your homework.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submit\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output.\n",
    "**Please save before submitting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to create a pdf for your reference."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW3-4_ConvNets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
